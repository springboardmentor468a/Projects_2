{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This cell is for installing the COCO API, which helps read the annotation files.\n# It's typically pre-installed on Kaggle, but this ensures it's available.\n!pip install pycocotools --quiet\n\nimport os\nimport json\nimport random\nimport numpy as np\nfrom pycocotools.coco import COCO\nfrom PIL import Image, ImageDraw\nimport matplotlib.pyplot as plt\nimport cv2\nimport numpy as np\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tqdm import tqdm\n\n\nprint(\"Libraries imported successfully.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the paths to the dataset\n# This path is standard for this dataset on Kaggle\nBASE_PATH = '/kaggle/input/coco-2017-dataset/coco2017/'\nIMG_PATH = os.path.join(BASE_PATH, 'train2017')\nANN_PATH = os.path.join(BASE_PATH,r 'annotations', 'instances_train2017.json')\n\n# Initialize the COCO API for instance annotations\nprint(\"Loading annotations... This may take a moment.\")\ncoco = COCO(ANN_PATH)\nprint(\"Annotations loaded.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get all image IDs from the training set\nall_img_ids = coco.getImgIds()\nprint(f\"Total images in the training set: {len(all_img_ids)}\")\n\n# Collect the first 20,000 image IDs\nimage_ids_20k = all_img_ids[:20000]\n\nprint(f\"Collected {len(image_ids_20k)} image IDs to work with.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Select 50 images from our 20k list for visualization\n# We'll take the first 50\nimages_to_visualize = image_ids_20k[:50]\n\nprint(f\"Starting visualization for {len(images_to_visualize)} images...\")\n\nfor img_id in images_to_visualize:\n    # 1. Load Image Info\n    img_info = coco.loadImgs([img_id])[0]\n    img_path = os.path.join(IMG_PATH, img_info['file_name'])\n    \n    # 2. Load Original Image (as RGB)\n    original_image = Image.open(img_path).convert('RGB')\n    \n    # 3. Get all annotations for this image\n    ann_ids = coco.getAnnIds(imgIds=img_id)\n    annotations = coco.loadAnns(ann_ids)\n    \n    # 4. Create the segmentation mask using numpy and coco.annToMask\n    # This is more robust as it handles both polygon and RLE formats.\n    \n    # Create a blank numpy array for the combined mask\n    combined_mask_np = np.zeros((img_info['height'], img_info['width']), dtype=np.uint8)\n    \n    # Iterate over all annotations (all subjects) for this image\n    for ann in annotations:\n        # coco.annToMask returns a binary 0/1 numpy array\n        # This function correctly handles both RLE and polygon formats\n        single_ann_mask_np = coco.annToMask(ann)\n        \n        # Combine masks using logical OR\n        # This adds the current object's mask to the combined mask\n        combined_mask_np = np.logical_or(combined_mask_np, single_ann_mask_np).astype(np.uint8)\n\n    # Convert the final combined numpy mask (0/1) to a PIL Image (0/255)\n    # We multiply by 255 to make the mask visible (white) for PIL\n    mask_pil = Image.fromarray(combined_mask_np * 255, 'L')\n    \n    # 5. Create the final \"Subject Isolated\" image\n    # This creates a black RGB background\n    black_bg = Image.new('RGB', original_image.size, (0, 0, 0))\n    \n    # Use the PIL mask to composite the images\n    # Where mask_pil is 255 (white), pixels from original_image are used.\n    # Where mask_pil is 0 (black), pixels from black_bg are used.\n    masked_image = Image.composite(original_image, black_bg, mask_pil)\n    \n    # 6. Visualize side-by-side\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n    \n    # Original Image\n    ax1.imshow(original_image)\n    ax1.set_title(f\"Original Image: {img_info['file_name']}\")\n    ax1.axis('off')\n    \n    # Masked Image (Subject Isolated)\n    ax2.imshow(masked_image)\n    ax2.set_title(\"Masked Image (Subject Isolated)\")\n    ax2.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\nprint(\"Visualization complete.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def resize_image(image, size=(224, 224)):\n    return cv2.resize(image, size)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def normalize_image(image):\n    return image / 255.0\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"datagen = ImageDataGenerator(\n    rotation_range=20,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    shear_range=0.1,\n    zoom_range=0.1,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 1: imports & params\nimport os\nimport glob\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport matplotlib.pyplot as plt\n# Params - change as needed\ntrain_path = \"/kaggle/input/coco-2017-dataset/coco2017/train2017\"\nval_path   = \"/kaggle/input/coco-2017-dataset/coco2017/val2017\"  # optional\nuse_val = False           # set True to include val images\nmax_images = 20000        # how many source images to process (limit)\nsave_sample_count = 20    # how many processed images to save for inspection\noutput_image_folder = \"/kaggle/working/processed_images\"\noutput_npy_path = \"/kaggle/working/processed_images_all.npy\"  # optional: set None to skip\nos.makedirs(output_image_folder, exist_ok=True)\nseed = 42\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 2: preprocessing helpers\ndef resize_image(image, size=(224, 224)):\n    return cv2.resize(image, size, interpolation=cv2.INTER_AREA)\n\ndef normalize_image(image):\n    # convert to float32 and scale to [0,1]\n    return image.astype(np.float32) / 255.0\n\n# Keras ImageDataGenerator for augmentation\ndatagen = ImageDataGenerator(\n    rotation_range=20,\n    width_shift_range=0.08,\n    height_shift_range=0.08,\n    shear_range=0.05,\n    zoom_range=0.08,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\nnp.random.seed(seed)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 3: gather filenames\ndef get_image_list(train_path, use_val=False, val_path=None, max_images=20000):\n    files = sorted(glob.glob(os.path.join(train_path, \"*.jpg\")) + glob.glob(os.path.join(train_path, \"*.png\")))\n    if use_val and val_path:\n        files += sorted(glob.glob(os.path.join(val_path, \"*.jpg\")) + glob.glob(os.path.join(val_path, \"*.png\")))\n    # deduplicate and limit\n    files = list(dict.fromkeys(files))[:max_images]\n    return files\n\nimage_files = get_image_list(train_path, use_val=use_val, val_path=val_path, max_images=max_images)\nprint(\"Files to process:\", len(image_files))\nif len(image_files) == 0:\n    print(\"No image files found â€” check paths. Example listing:\", os.listdir(os.path.dirname(train_path))[:20])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"SAVE_PATH = \"/kaggle/working/processed_images\"\nos.makedirs(SAVE_PATH, exist_ok=True)\n\nprint(\"Saving processed masked images...\")\n\ncount = 0\nfor img_id in tqdm(image_ids_20k[:5000]):  # save first 5000\n    img_info = coco.loadImgs([img_id])[0]\n    img_path = os.path.join(IMG_PATH, img_info['file_name'])\n    original_image = Image.open(img_path).convert('RGB')\n\n    ann_ids = coco.getAnnIds(imgIds=img_id)\n    annotations = coco.loadAnns(ann_ids)\n\n    mask = np.zeros((img_info['height'], img_info['width']), dtype=np.uint8)\n\n    for ann in annotations:\n        mask = np.logical_or(mask, coco.annToMask(ann)).astype(np.uint8)\n\n    masked = Image.composite(original_image, Image.new('RGB', original_image.size, (0,0,0)), Image.fromarray(mask * 255, 'L'))\n    masked = masked.resize((256, 256))\n    \n    save_name = f\"masked_{img_id}.jpg\"\n    masked.save(os.path.join(SAVE_PATH, save_name))\n    count += 1\n\nprint(f\"Saved {count} processed masked images.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport random\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\n# Path to processed images (adjust if needed)\nPROCESSED_PATH = \"/kaggle/working/processed_images\"\n\n# Get list of processed image files\nall_images = sorted(os.listdir(PROCESSED_PATH))\nprint(f\"Total processed images found: {len(all_images)}\")\n\n# Select only 5000 for the pipeline\npipeline_images = all_images[:5000]\nprint(f\"Using {len(pipeline_images)} images for pipeline building...\")\n\n# Example pipeline steps: resize, normalization, augmentation\ndef process_image(img_path):\n    img = cv2.imread(os.path.join(PROCESSED_PATH, img_path))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    \n    # Resize to a smaller shape (adjust if needed)\n    img_resized = cv2.resize(img, (256, 256))\n\n    # Normalization\n    img_norm = img_resized / 255.0\n\n    # Simple Augmentation: horizontal flip (example)\n    if random.random() > 0.5:\n        img_aug = cv2.flip(img_norm, 1)\n    else:\n        img_aug = img_norm\n    \n    return img_aug\n\nprocessed_samples = []\n\nfor img_file in tqdm(pipeline_images, desc=\"Building pipeline\"):\n    processed_samples.append(process_image(img_file))\n\nprint(\"âœ… Pipeline processing complete.\")\n\n# ---- Display 50 random sample images ----\nsample_images = random.sample(processed_samples, 50)\nplt.figure(figsize=(15, 15))\nfor i, img in enumerate(sample_images):\n    plt.subplot(5, 10, i+1)\n    plt.imshow(img)\n    plt.axis('off')\nplt.suptitle(\"Sample 50 Augmented + Resized Images\", fontsize=16)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nprint(\"Input root:\", os.listdir(\"/kaggle/input\"))\n\nprint(\"COCO dataset:\", os.listdir(\"/kaggle/input/coco-2017-dataset\"))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nroot = \"/kaggle/input/coco-2017-dataset/coco2017\"\nprint(\"Root contents:\", os.listdir(root))\n\n# Check for expected subfolders\nfor sub in [\"train2017\", \"val2017\", \"annotations\"]:\n    path = os.path.join(root, sub)\n    print(sub, \"exists:\", os.path.exists(path))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nfrom tqdm import tqdm\n\nINPUT_DIR = \"/kaggle/working/processed_images\"      # RGB augmented images\nOUTPUT_DIR = \"/kaggle/working/processed_images_gray\"  # Grayscale output folder\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nimage_files = sorted(os.listdir(INPUT_DIR))\nprint(f\"Total RGB processed images found: {len(image_files)}\")\n\nfor img_name in tqdm(image_files, desc=\"Converting to Grayscale\"):\n    img_path = os.path.join(INPUT_DIR, img_name)\n    \n    # Load image\n    img = cv2.imread(img_path)\n    if img is None:\n        continue\n    \n    # Convert BGR â†’ Grayscale\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n    # Optional: resize if needed\n    # gray = cv2.resize(gray, (224, 224))\n\n    # Save grayscale image\n    save_name = os.path.splitext(img_name)[0] + \"_gray.jpg\"\n    cv2.imwrite(os.path.join(OUTPUT_DIR, save_name), gray)\n\nprint(\"âœ… Grayscale conversion completed!\")\nprint(f\"Grayscale images saved to: {OUTPUT_DIR}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport random\n\nsample_files = random.sample(image_files, min(20, len(image_files)))\n\nplt.figure(figsize=(12, 8))\nfor i, name in enumerate(sample_files):\n    img = cv2.imread(os.path.join(OUTPUT_DIR, name.replace(\".jpg\", \"_gray.jpg\")))\n    plt.subplot(4, 5, i+1)\n    plt.imshow(img, cmap=\"gray\")\n    plt.axis(\"off\")\n\nplt.suptitle(\"Random Grayscale Converted Images\", fontsize=16)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom pycocotools.coco import COCO\nfrom PIL import Image\nfrom tqdm import tqdm\nimport cv2\n\n# Input masked image path\nIMG_DIR = \"/kaggle/working/processed_images\"\n\n# Output mask path\nMASK_DIR = \"/kaggle/working/processed_masks\"\nos.makedirs(MASK_DIR, exist_ok=True)\n\n# COCO path\nBASE_PATH = '/kaggle/input/coco-2017-dataset/coco2017/'\nANN_PATH = os.path.join(BASE_PATH, 'annotations', 'instances_train2017.json')\ncoco = COCO(ANN_PATH)\n\n# Extract image IDs from filenames\nfiles = sorted(os.listdir(IMG_DIR))\n\nprint(\"Saving binary masks...\")\n\nfor file in tqdm(files):\n    if not file.startswith(\"masked_\") or not file.endswith(\".jpg\"):\n        continue\n\n    img_id = int(file.replace(\"masked_\", \"\").replace(\".jpg\", \"\"))  # recover COCO image id\n    img_info = coco.loadImgs([img_id])[0]\n\n    ann_ids = coco.getAnnIds(imgIds=img_id)\n    anns = coco.loadAnns(ann_ids)\n\n    mask = np.zeros((img_info['height'], img_info['width']), dtype=np.uint8)\n\n    for ann in anns:\n        mask = np.logical_or(mask, coco.annToMask(ann)).astype(np.uint8)\n\n    # Resize mask to 256x256 same as masked images\n    mask = cv2.resize(mask, (256, 256), interpolation=cv2.INTER_NEAREST)\n    mask = (mask > 0).astype(\"uint8\")\n\n    np.save(os.path.join(MASK_DIR, file.replace(\".jpg\", \".npy\")), mask)\n\nprint(\"âœ… Mask creation complete!\")\nprint(\"Masks saved in:\", MASK_DIR)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Images:\", len(os.listdir(\"/kaggle/working/processed_images\")))\nprint(\"Masks :\", len(os.listdir(\"/kaggle/working/processed_masks\")))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"IMG_DIR = \"/kaggle/working/processed_images\"\nMASK_DIR = \"/kaggle/working/processed_masks\"\nIMG_SIZE = (256, 256)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport os\nfrom tensorflow.keras.utils import Sequence\nfrom sklearn.model_selection import train_test_split\n\nfiles = sorted([f for f in os.listdir(IMG_DIR) if f.endswith(\".jpg\")])\n\ntrain_files, val_files = train_test_split(files, test_size=0.1, random_state=42)\n\nclass CocoDataGen(Sequence):\n    def __init__(self, file_list, batch_size=8, shuffle=True):\n        self.file_list = file_list\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.on_epoch_end()\n\n    def __len__(self):\n        return len(self.file_list) // self.batch_size\n\n    def on_epoch_end(self):\n        if self.shuffle:\n            np.random.shuffle(self.file_list)\n\n    def __getitem__(self, idx):\n        batch_files = self.file_list[idx*self.batch_size:(idx+1)*self.batch_size]\n        X, Y = [], []\n        for f in batch_files:\n            img = cv2.imread(os.path.join(IMG_DIR, f), cv2.IMREAD_GRAYSCALE)\n            img = img.astype(\"float32\") / 255.0\n            img = np.expand_dims(img, -1)\n\n            mask = np.load(os.path.join(MASK_DIR, f.replace(\".jpg\", \".npy\")))\n            mask = np.expand_dims(mask.astype(\"float32\"), -1)\n\n            X.append(img)\n            Y.append(mask)\n\n        return np.array(X), np.array(Y)\n\ntrain_gen = CocoDataGen(train_files, batch_size=8)\nval_gen   = CocoDataGen(val_files,   batch_size=8, shuffle=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model\n\ndef build_unet(input_shape=(256, 256, 1)):\n    inputs = Input(input_shape)\n\n    def conv_block(x, filters):\n        x = Conv2D(filters, 3, activation=\"relu\", padding=\"same\")(x)\n        x = Conv2D(filters, 3, activation=\"relu\", padding=\"same\")(x)\n        return x\n\n    c1 = conv_block(inputs, 16); p1 = MaxPooling2D()(c1)\n    c2 = conv_block(p1, 32); p2 = MaxPooling2D()(c2)\n    c3 = conv_block(p2, 64); p3 = MaxPooling2D()(c3)\n    c4 = conv_block(p3, 128); p4 = MaxPooling2D()(c4)\n\n    bn = conv_block(p4, 256)\n\n    u4 = UpSampling2D()(bn); u4 = Concatenate()([u4, c4]); c5 = conv_block(u4, 128)\n    u3 = UpSampling2D()(c5); u3 = Concatenate()([u3, c3]); c6 = conv_block(u3, 64)\n    u2 = UpSampling2D()(c6); u2 = Concatenate()([u2, c2]); c7 = conv_block(u2, 32)\n    u1 = UpSampling2D()(c7); u1 = Concatenate()([u1, c1]); c8 = conv_block(u1, 16)\n\n    outputs = Conv2D(1, 1, activation=\"sigmoid\")(c8)\n\n    return Model(inputs, outputs)\n\nmodel = build_unet()\nmodel.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\nmodel.summary()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(\n    train_gen,\n    validation_data=val_gen,\n    epochs=10,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n    ]\n)\n\nmodel.save(\"/kaggle/working/unet_coco_allclasses.h5\")\nprint(\"ðŸŽ‰ Model training complete & saved!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\nimport random\nimport os\n\n# ---------------------------\n# 1ï¸âƒ£ Load the saved model\n# ---------------------------\nmodel_path = \"/kaggle/working/unet_coco_allclasses.h5\"\nmodel = tf.keras.models.load_model(model_path)\nprint(\"Model loaded successfully!\")\n\n# ---------------------------\n# 2ï¸âƒ£ Select random samples from validation set\n# ---------------------------\nnum_samples = 3  # how many predictions to visualize\nsample_files = random.sample(val_files, num_samples)\n\n# ---------------------------\n# 3ï¸âƒ£ Helper function to load image & mask\n# ---------------------------\ndef load_image_mask(img_filename):\n    img_path = os.path.join(IMG_DIR, img_filename)\n    mask_path = os.path.join(MASK_DIR, img_filename.replace(\".jpg\", \".png\"))  # adjust extension if needed\n\n    # Load image\n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img_resized = cv2.resize(img, (IMG_SIZE, IMG_SIZE)) / 255.0\n\n    # Load mask\n    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n    mask_resized = cv2.resize(mask, (IMG_SIZE, IMG_SIZE)) / 255.0\n    mask_resized = np.expand_dims(mask_resized, axis=-1)\n\n    return img, img_resized, mask, mask_resized\n\n# ---------------------------\n# 4ï¸âƒ£ Predict & Plot results\n# ---------------------------\nfor img_file in sample_files:\n    orig_img, img_input, orig_mask, mask_input = load_image_mask(img_file)\n\n    pred_mask = model.predict(np.expand_dims(img_input, axis=0))[0]\n    pred_mask = (pred_mask > 0.5).astype(np.uint8)  # thresholding\n\n    # Plotting\n    plt.figure(figsize=(12,4))\n    \n    plt.subplot(1, 3, 1)\n    plt.title(\"Original Image\")\n    plt.imshow(orig_img)\n    plt.axis(\"off\")\n\n    plt.subplot(1, 3, 2)\n    plt.title(\"Ground Truth Mask\")\n    plt.imshow(orig_mask, cmap=\"gray\")\n    plt.axis(\"off\")\n\n    plt.subplot(1, 3, 3)\n    plt.title(\"Predicted Mask\")\n    plt.imshow(pred_mask.squeeze(), cmap=\"gray\")\n    plt.axis(\"off\")\n\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}